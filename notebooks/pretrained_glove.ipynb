{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook utilise un modèle glove 6B préentrainé sur Wikipedia 2014 et Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download) téléchargé ici: http://nlp.stanford.edu/data/glove.6B.zip\n",
    "Notebook inspiré de https://www.kaggle.com/jhoward/improved-lstm-baseline-glove-dropout/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs \n",
    "import numpy as np\n",
    "import pandas as pd, configparser\n",
    "\n",
    "# keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, GlobalMaxPool1D, Bidirectional, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('../config.cfg')\n",
    "EMBEDDING_FILE=config['FILES']['GLOVE']\n",
    "TRAIN_DATA_FILE=config['FILES']['TRAIN']\n",
    "TEST_DATA_FILE=config['FILES']['TEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données sous forme de dataframes pandas\n",
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "# Définition des différents labels disponibles sous forme de liste\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "y = train[list_classes].values\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # taille du vecteur\n",
    "max_features = 20000 # nombre de mots uniques utilisés (i.e nombre de lignes dans le vecteur d'embedding)\n",
    "maxlen = 100 # nombre maximum de mots à considérer dans un commentaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# le modèle pré-entrainé contient 3 millions de mots contre 20000 pour notre jeu de données.\n",
    "vocabulary_size=min(len(word_index)+1,max_features)\n",
    "\n",
    "# la matrice d'embedding représente le vocabulaire final à utiliser. \n",
    "# On ne gardera que 20000 vecteurs du modèle pré-entrainé. Leur longueur est de 300.\n",
    "embedding_matrix = np.zeros((vocabulary_size, embed_size))\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i>=max_features:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = embeddings_index[word]\n",
    "        if len(embedding_vector) == 300:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        elif len(embedding_vector) > 300:\n",
    "            embedding_matrix[i] = embedding_vector[0:300]\n",
    "        else:\n",
    "            while len(embedding_vector) < 300:\n",
    "                embedding_vector.append(0)\n",
    "    except KeyError:\n",
    "        vec = np.zeros(embed_size)\n",
    "        embedding_matrix[i]=vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1134s 8ms/step - loss: 0.0546 - acc: 0.9806 - val_loss: 0.0459 - val_acc: 0.9830\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1090s 8ms/step - loss: 0.0407 - acc: 0.9844 - val_loss: 0.0459 - val_acc: 0.9828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7a233f1fd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y, batch_size=32, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = pd.read_csv(config['FILES']['LABEL'])\n",
    "test_label_strip = test_label[test_label.toxic != -1]\n",
    "yt = test_label_strip[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63978/63978 [==============================] - 70s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06264560487349713, 0.9744313591276889]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_te[test_label_strip.index], yt, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_t = model.predict(X_te, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8918847103691894\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(yt, y_pred_t[test_label_strip.index].round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Traitement du label toxic\n",
      "L'accuracy du jeu test est 0.9332114164243959\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[54891  2997]\n",
      " [ 1276  4814]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96     57888\n",
      "           1       0.62      0.79      0.69      6090\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     63978\n",
      "   macro avg       0.80      0.87      0.83     63978\n",
      "weighted avg       0.94      0.93      0.94     63978\n",
      "\n",
      "... Traitement du label severe_toxic\n",
      "L'accuracy du jeu test est 0.9926693550908124\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[63327   284]\n",
      " [  185   182]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     63611\n",
      "           1       0.39      0.50      0.44       367\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     63978\n",
      "   macro avg       0.69      0.75      0.72     63978\n",
      "weighted avg       0.99      0.99      0.99     63978\n",
      "\n",
      "... Traitement du label obscene\n",
      "L'accuracy du jeu test est 0.9651442683422427\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[59184  1103]\n",
      " [ 1127  2564]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     60287\n",
      "           1       0.70      0.69      0.70      3691\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     63978\n",
      "   macro avg       0.84      0.84      0.84     63978\n",
      "weighted avg       0.97      0.97      0.97     63978\n",
      "\n",
      "... Traitement du label threat\n",
      "L'accuracy du jeu test est 0.9967801431742161\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[63760     7]\n",
      " [  199    12]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     63767\n",
      "           1       0.63      0.06      0.10       211\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     63978\n",
      "   macro avg       0.81      0.53      0.55     63978\n",
      "weighted avg       1.00      1.00      1.00     63978\n",
      "\n",
      "... Traitement du label insult\n",
      "L'accuracy du jeu test est 0.966785457501016\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[59748   803]\n",
      " [ 1322  2105]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     60551\n",
      "           1       0.72      0.61      0.66      3427\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     63978\n",
      "   macro avg       0.85      0.80      0.82     63978\n",
      "weighted avg       0.96      0.97      0.97     63978\n",
      "\n",
      "... Traitement du label identity_hate\n",
      "L'accuracy du jeu test est 0.9919972490543625\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[63078   188]\n",
      " [  324   388]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     63266\n",
      "           1       0.67      0.54      0.60       712\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     63978\n",
      "   macro avg       0.83      0.77      0.80     63978\n",
      "weighted avg       0.99      0.99      0.99     63978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification pour chaque label de façon indépendante\n",
    "for label in range(0,6):\n",
    "    print('... Traitement du label {}'.format(list_classes[label]))\n",
    "    # On calcule l'accuracy des prédictions\n",
    "    acc = accuracy_score(yt[:,label], y_pred_t[test_label_strip.index, label].round())\n",
    "    print('L\\'accuracy du jeu test est {}'.format(acc))\n",
    "    print('Matrice de confusion sur le jeu test:')\n",
    "    print(confusion_matrix(yt[:,label], y_pred_t[test_label_strip.index, label].round()))\n",
    "    print('Rapport : ')\n",
    "    print(classification_report(yt[:,label], y_pred_t[test_label_strip.index, label].round()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
