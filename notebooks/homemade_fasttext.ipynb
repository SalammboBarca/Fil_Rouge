{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs \n",
    "import numpy as np\n",
    "import pandas as pd, configparser\n",
    "\n",
    "# gensim\n",
    "from gensim.models import FastText\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "# keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, GlobalMaxPool1D, Bidirectional, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "#nltk \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('../config.cfg')\n",
    "TRAIN_DATA_FILE=config['FILES']['TRAIN']\n",
    "TEST_DATA_FILE=config['FILES']['TEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données sous forme de dataframes pandas\n",
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "# Définition des différents labels disponibles sous forme de liste\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "y = train[list_classes].values\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour créer un modèle Word2vec, il faut transformer le texte en liste de mots (tokens). \n",
    "# On retire les stopWords et la ponctuaction pour essayer de coller au modèle Google\n",
    "sentences = []\n",
    "stopWords = set(stopwords.words('english'))\n",
    "all_comment = list_sentences_train.tolist() + list_sentences_test.tolist()\n",
    "for comment in all_comment:\n",
    "    comment = ' '.join(re.split('\\n',comment.lower()))\n",
    "    comment = re.sub(r'[^a-zA-Z0-9\\s:]', '', comment)\n",
    "    words = word_tokenize(comment)\n",
    "    wordsFiltered = []\n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    sentences.append(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(size=300, sentences=sentences, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = get_tmpfile(\"fasttext.model\")\n",
    "model.save(fname)\n",
    "model = FastText.load(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/models/fasttext.html\n",
    "https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # taille du vecteur\n",
    "max_features = 20000 # nombre de mots uniques utilisés (i.e nombre de lignes dans le vecteur d'embedding)\n",
    "maxlen = 100 # nombre maximum de mots à considérer dans un commentaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A des fins de comparaison, on utilise le même tokenizer que pour le notebook Google\n",
    "\n",
    "# On garde les mots les plus fréquents dans le jeu d'entrainement pour établir notre liste de tokens.\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "# Calcul des mots les plus fréquents\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "# Indexation des jeux d'entrainement et test, conversion du texte en séquence d'indexes\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les commentaires ont une longueur variable qui peut être inférieure à maxlen\n",
    "# On complète donc la séquence avec des 0, par défaut au début de la séquence\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# le modèle pré-entrainé contient 3 millions de mots contre 20000 pour notre jeu de données.\n",
    "vocabulary_size=min(len(word_index)+1,max_features)\n",
    "\n",
    "# la matrice d'embedding représente le vocabulaire final à utiliser. \n",
    "# On ne gardera que 20000 vecteurs du modèle pré-entrainé. Leur longueur est de 300.\n",
    "embedding_matrix = np.zeros((vocabulary_size, embed_size))\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i>=max_features:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector    \n",
    "    except KeyError:\n",
    "        vec = np.zeros(embed_size)\n",
    "        embedding_matrix[i]=vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 734s 5ms/step - loss: 0.0553 - acc: 0.9806 - val_loss: 0.0477 - val_acc: 0.9821\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 715s 5ms/step - loss: 0.0447 - acc: 0.9831 - val_loss: 0.0460 - val_acc: 0.9828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f33e7fabda0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y, batch_size=32, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = pd.read_csv(config['FILES']['LABEL'])\n",
    "test_label_strip = test_label[test_label.toxic != -1]\n",
    "yt = test_label_strip[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63978/63978 [==============================] - 48s 756us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06498602694861513, 0.9732695033466862]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_te[test_label_strip.index], yt, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_t = model.predict(X_te, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8869924036387509\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(yt, y_pred_t[test_label_strip.index].round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Traitement du label toxic\n",
      "L'accuracy du jeu test est 0.929835255869205\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[54620  3268]\n",
      " [ 1221  4869]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96     57888\n",
      "           1       0.60      0.80      0.68      6090\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     63978\n",
      "   macro avg       0.79      0.87      0.82     63978\n",
      "weighted avg       0.94      0.93      0.93     63978\n",
      "\n",
      "... Traitement du label severe_toxic\n",
      "L'accuracy du jeu test est 0.994248022757823\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[63570    41]\n",
      " [  327    40]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     63611\n",
      "           1       0.49      0.11      0.18       367\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     63978\n",
      "   macro avg       0.74      0.55      0.59     63978\n",
      "weighted avg       0.99      0.99      0.99     63978\n",
      "\n",
      "... Traitement du label obscene\n",
      "L'accuracy du jeu test est 0.9624714745693832\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[58963  1324]\n",
      " [ 1077  2614]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     60287\n",
      "           1       0.66      0.71      0.69      3691\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     63978\n",
      "   macro avg       0.82      0.84      0.83     63978\n",
      "weighted avg       0.96      0.96      0.96     63978\n",
      "\n",
      "... Traitement du label threat\n",
      "L'accuracy du jeu test est 0.9966394698177499\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[63761     6]\n",
      " [  209     2]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     63767\n",
      "           1       0.25      0.01      0.02       211\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     63978\n",
      "   macro avg       0.62      0.50      0.51     63978\n",
      "weighted avg       0.99      1.00      1.00     63978\n",
      "\n",
      "... Traitement du label insult\n",
      "L'accuracy du jeu test est 0.9653943543092938\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[59627   924]\n",
      " [ 1290  2137]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     60551\n",
      "           1       0.70      0.62      0.66      3427\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     63978\n",
      "   macro avg       0.84      0.80      0.82     63978\n",
      "weighted avg       0.96      0.97      0.96     63978\n",
      "\n",
      "... Traitement du label identity_hate\n",
      "L'accuracy du jeu test est 0.9910281659320391\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[63159   107]\n",
      " [  467   245]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     63266\n",
      "           1       0.70      0.34      0.46       712\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     63978\n",
      "   macro avg       0.84      0.67      0.73     63978\n",
      "weighted avg       0.99      0.99      0.99     63978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification pour chaque label de façon indépendante\n",
    "for label in range(0,6):\n",
    "    print('... Traitement du label {}'.format(list_classes[label]))\n",
    "    # On calcule l'accuracy des prédictions\n",
    "    acc = accuracy_score(yt[:,label], y_pred_t[test_label_strip.index, label].round())\n",
    "    print('L\\'accuracy du jeu test est {}'.format(acc))\n",
    "    print('Matrice de confusion sur le jeu test:')\n",
    "    print(confusion_matrix(yt[:,label], y_pred_t[test_label_strip.index, label].round()))\n",
    "    print('Rapport : ')\n",
    "    print(classification_report(yt[:,label], y_pred_t[test_label_strip.index, label].round()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
