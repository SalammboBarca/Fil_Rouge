{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import configparser\n",
    "\n",
    "# keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, GlobalMaxPool1D, Bidirectional, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "# gensim\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report \n",
    "\n",
    "#nltk \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('../config.cfg')\n",
    "EMBEDDING_FILE=config['FILES']['WORD']\n",
    "TRAIN_DATA_FILE=config['FILES']['TRAIN']\n",
    "TEST_DATA_FILE=config['FILES']['TEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données sous forme de dataframes pandas\n",
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "# Définition des différents labels disponibles sous forme de liste\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "y = train[list_classes].values\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation des commentaires pour le modèle Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour créer un modèle Word2vec, il faut transformer le texte en liste de mots (tokens). \n",
    "# On retire les stopWords et la ponctuaction pour essayer de coller au modèle Google\n",
    "sentences = []\n",
    "stopWords = set(stopwords.words('english'))\n",
    "all_comment = list_sentences_train.tolist() + list_sentences_test.tolist()\n",
    "for comment in all_comment:\n",
    "    comment = ' '.join(re.split('\\n',comment.lower()))\n",
    "    comment = re.sub(r'[^a-zA-Z0-9\\s:]', '', comment)\n",
    "    words = word_tokenize(comment)\n",
    "    wordsFiltered = []\n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    sentences.append(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312735"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pour maximiser le vocabulaire du modèle, on utilise le jeu d'entrainement et test\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du modèle Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On choisit une taille de vecteur 300 comme le modèle Google\n",
    "model = Word2Vec(sentences, size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde et chargement du modèle Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = get_tmpfile(\"vectors.kv\")\n",
    "word_vectors.save(fname)\n",
    "word_vectors = KeyedVectors.load(fname, mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':', 'article', 'page', 'wikipedia', 'would', 'like', 'one', 'talk', 'please', 'dont', 'see', 'think', 'im', 'also', 'know', 'fuck', 'people', 'articles', 'edit', 'use']\n"
     ]
    }
   ],
   "source": [
    "# get the most common words\n",
    "print(word_vectors.index2word[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # taille du vecteur\n",
    "max_features = 20000 # nombre de mots uniques utilisés (i.e nombre de lignes dans le vecteur d'embedding)\n",
    "maxlen = 100 # nombre maximum de mots à considérer dans un commentaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation pour la classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A des fins de comparaison, on utilise le même tokenizer que pour le notebook Google\n",
    "\n",
    "# On garde les mots les plus fréquents dans le jeu d'entrainement pour établir notre liste de tokens.\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "# Calcul des mots les plus fréquents\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "# Indexation des jeux d'entrainement et test, conversion du texte en séquence d'indexes\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les commentaires ont une longueur variable qui peut être inférieure à maxlen\n",
    "# On complète donc la séquence avec des 0, par défaut au début de la séquence\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# le modèle pré-entrainé contient 3 millions de mots contre 20000 pour notre jeu de données.\n",
    "vocabulary_size=min(len(word_index)+1,max_features)\n",
    "\n",
    "# la matrice d'embedding représente le vocabulaire final à utiliser. \n",
    "# On ne gardera que 20000 vecteurs du modèle pré-entrainé. Leur longueur est de 300.\n",
    "embedding_matrix = np.zeros((vocabulary_size, embed_size))\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i>=max_features:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector    \n",
    "    except KeyError:\n",
    "        vec = np.zeros(embed_size)\n",
    "        embedding_matrix[i]=vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 739s 5ms/step - loss: 0.0531 - acc: 0.9809 - val_loss: 0.0454 - val_acc: 0.9831\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 735s 5ms/step - loss: 0.0417 - acc: 0.9840 - val_loss: 0.0455 - val_acc: 0.9831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f893aaa8c18>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y, batch_size=32, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = pd.read_csv(config['FILES']['LABEL'])\n",
    "test_label_strip = test_label[test_label.toxic != -1]\n",
    "yt = test_label_strip[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63978/63978 [==============================] - 53s 832us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07073896178930458, 0.9705133489816529]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_te[test_label_strip.index], yt, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_t = model.predict(X_te, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.873612804401513\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(yt, y_pred_t[test_label_strip.index].round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Traitement du label toxic\n",
      "L'accuracy du jeu test est 0.9177686079589858\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[53443  4445]\n",
      " [  816  5274]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95     57888\n",
      "           1       0.54      0.87      0.67      6090\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     63978\n",
      "   macro avg       0.76      0.89      0.81     63978\n",
      "weighted avg       0.94      0.92      0.93     63978\n",
      "\n",
      "... Traitement du label severe_toxic\n",
      "L'accuracy du jeu test est 0.9934352433649067\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[63464   147]\n",
      " [  273    94]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     63611\n",
      "           1       0.39      0.26      0.31       367\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     63978\n",
      "   macro avg       0.69      0.63      0.65     63978\n",
      "weighted avg       0.99      0.99      0.99     63978\n",
      "\n",
      "... Traitement du label obscene\n",
      "L'accuracy du jeu test est 0.9624558441964425\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[58852  1435]\n",
      " [  967  2724]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     60287\n",
      "           1       0.65      0.74      0.69      3691\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     63978\n",
      "   macro avg       0.82      0.86      0.84     63978\n",
      "weighted avg       0.96      0.96      0.96     63978\n",
      "\n",
      "... Traitement du label threat\n",
      "L'accuracy du jeu test est 0.9966707305636312\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[63755    12]\n",
      " [  201    10]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     63767\n",
      "           1       0.45      0.05      0.09       211\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     63978\n",
      "   macro avg       0.73      0.52      0.54     63978\n",
      "weighted avg       1.00      1.00      1.00     63978\n",
      "\n",
      "... Traitement du label insult\n",
      "L'accuracy du jeu test est 0.9613304573447122\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[59052  1499]\n",
      " [  975  2452]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     60551\n",
      "           1       0.62      0.72      0.66      3427\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     63978\n",
      "   macro avg       0.80      0.85      0.82     63978\n",
      "weighted avg       0.96      0.96      0.96     63978\n",
      "\n",
      "... Traitement du label identity_hate\n",
      "L'accuracy du jeu test est 0.9914189252555566\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[63081   185]\n",
      " [  364   348]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     63266\n",
      "           1       0.65      0.49      0.56       712\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     63978\n",
      "   macro avg       0.82      0.74      0.78     63978\n",
      "weighted avg       0.99      0.99      0.99     63978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification pour chaque label de façon indépendante\n",
    "for label in range(0,6):\n",
    "    print('... Traitement du label {}'.format(list_classes[label]))\n",
    "    # On calcule l'accuracy des prédictions\n",
    "    acc = accuracy_score(yt[:,label], y_pred_t[test_label_strip.index, label].round())\n",
    "    print('L\\'accuracy du jeu test est {}'.format(acc))\n",
    "    print('Matrice de confusion sur le jeu test:')\n",
    "    print(confusion_matrix(yt[:,label], y_pred_t[test_label_strip.index, label].round()))\n",
    "    print('Rapport : ')\n",
    "    print(classification_report(yt[:,label], y_pred_t[test_label_strip.index, label].round()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
