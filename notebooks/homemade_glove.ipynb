{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "#keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, GlobalMaxPool1D, Bidirectional, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('../config.cfg')\n",
    "train = pd.read_csv(config['FILES']['TRAIN'])\n",
    "test = pd.read_csv(config['FILES']['TEST'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des différents labels disponibles sous forme de liste\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "y = train[list_classes].values\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation du modèle Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_sentences(df):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        df: dataframe a découper en phrases\n",
    "        \n",
    "    Cette fonction créé un fichier texte où toute la ponctuation a été retirée sauf les espaces et les :\n",
    "    Le texte est mis en minuscule et les sauts de lignes dans un commentaires sont remplacés par un espace. \n",
    "    \"\"\"\n",
    "    corpus = open(config['FILES']['GLOVE_DIR'] + 'custom_corpus', 'a')\n",
    "    for comment in df['comment_text']:\n",
    "        comment = ' '.join(re.split('\\n',comment.lower()))\n",
    "        comment = re.sub(r'[^a-zA-Z0-9\\s:]', '', comment)\n",
    "        corpus.write(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentences(train)\n",
    "my_sentences(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour créer le modèle Glove, on utilise les scripts bash mis à disposition par Glove sr https://github.com/stanfordnlp/GloVe\n",
    "On créé des vecteurs de taille 300 pour se comparer aux autres modèles précédemment utilisés. On obtient un fichier text (vectors.txt) avec 1 ligne = 1 mot et son vecteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE=config['FILES']['GLOVE_DIR'] + 'vectors.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # taille du vecteur\n",
    "max_features = 20000 # nombre de mots uniques utilisés (i.e nombre de lignes dans le vecteur d'embedding)\n",
    "maxlen = 100 # nombre maximum de mots à considérer dans un commentaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation pour la classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On garde les mots les plus fréquents dans le jeu d'entrainement pour établir notre liste de tokens.\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "# Calcul des mots les plus fréquents\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "# Indexation des jeux d'entrainement et test, conversion du texte en séquence d'indexes\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "# Les commentaires ont une longueur variable qui peut être inférieure à maxlen\n",
    "# On complète donc la séquence avec des 0, par défaut au début de la séquence\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lecture du fichier d'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "for o in open(EMBEDDING_FILE):\n",
    "    spt_o = o.strip().split()\n",
    "    try:\n",
    "        embeddings_index[spt_o[0]] = np.asarray(spt_o[1:], dtype='float32')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# le modèle pré-entrainé contient 3 millions de mots contre 20000 pour notre jeu de données.\n",
    "vocabulary_size=min(len(word_index)+1,max_features)\n",
    "\n",
    "# la matrice d'embedding représente le vocabulaire final à utiliser. \n",
    "# On ne gardera que 20000 vecteurs du modèle pré-entrainé. Leur longueur est de 300.\n",
    "embedding_matrix = np.zeros((vocabulary_size, embed_size))\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i>=max_features:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = embeddings_index[word]\n",
    "        if len(embedding_vector) == 300:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        elif len(embedding_vector) > 300:\n",
    "            embedding_matrix[i] = embedding_vector[0:300]\n",
    "        else:\n",
    "            while len(embedding_vector) < 300:\n",
    "                embedding_vector.append(0)\n",
    "    except KeyError:\n",
    "        vec = np.zeros(embed_size)\n",
    "        embedding_matrix[i]=vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 763s 5ms/step - loss: 0.0552 - acc: 0.9806 - val_loss: 0.0458 - val_acc: 0.9827\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1026s 7ms/step - loss: 0.0408 - acc: 0.9844 - val_loss: 0.0455 - val_acc: 0.9832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efbb5b6f828>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y, batch_size=32, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = pd.read_csv(config['FILES']['LABEL'])\n",
    "test_label_strip = test_label[test_label.toxic != -1]\n",
    "yt = test_label_strip[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63978/63978 [==============================] - 81s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07264734107505243, 0.9698125831206363]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_te[test_label_strip.index], yt, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_t = model.predict(X_te, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8749257557285317\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(yt, y_pred_t[test_label_strip.index].round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Traitement du label toxic\n",
      "L'accuracy du jeu test est 0.9181281065366219\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[53406  4482]\n",
      " [  756  5334]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95     57888\n",
      "           1       0.54      0.88      0.67      6090\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     63978\n",
      "   macro avg       0.76      0.90      0.81     63978\n",
      "weighted avg       0.94      0.92      0.93     63978\n",
      "\n",
      "... Traitement du label severe_toxic\n",
      "L'accuracy du jeu test est 0.9940135671637125\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[63534    77]\n",
      " [  306    61]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     63611\n",
      "           1       0.44      0.17      0.24       367\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     63978\n",
      "   macro avg       0.72      0.58      0.62     63978\n",
      "weighted avg       0.99      0.99      0.99     63978\n",
      "\n",
      "... Traitement du label obscene\n",
      "L'accuracy du jeu test est 0.9556097408484167\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[58099  2188]\n",
      " [  652  3039]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98     60287\n",
      "           1       0.58      0.82      0.68      3691\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     63978\n",
      "   macro avg       0.79      0.89      0.83     63978\n",
      "weighted avg       0.97      0.96      0.96     63978\n",
      "\n",
      "... Traitement du label threat\n",
      "L'accuracy du jeu test est 0.9965144268342243\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[63689    78]\n",
      " [  145    66]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     63767\n",
      "           1       0.46      0.31      0.37       211\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     63978\n",
      "   macro avg       0.73      0.66      0.69     63978\n",
      "weighted avg       1.00      1.00      1.00     63978\n",
      "\n",
      "... Traitement du label insult\n",
      "L'accuracy du jeu test est 0.9630341679952483\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[59238  1313]\n",
      " [ 1052  2375]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     60551\n",
      "           1       0.64      0.69      0.67      3427\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     63978\n",
      "   macro avg       0.81      0.84      0.82     63978\n",
      "weighted avg       0.96      0.96      0.96     63978\n",
      "\n",
      "... Traitement du label identity_hate\n",
      "L'accuracy du jeu test est 0.9915752289849635\n",
      "Matrice de confusion sur le jeu test:\n",
      "[[63097   169]\n",
      " [  370   342]]\n",
      "Rapport : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     63266\n",
      "           1       0.67      0.48      0.56       712\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     63978\n",
      "   macro avg       0.83      0.74      0.78     63978\n",
      "weighted avg       0.99      0.99      0.99     63978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification pour chaque label de façon indépendante\n",
    "for label in range(0,6):\n",
    "    print('... Traitement du label {}'.format(list_classes[label]))\n",
    "    # On calcule l'accuracy des prédictions\n",
    "    acc = accuracy_score(yt[:,label], y_pred_t[test_label_strip.index, label].round())\n",
    "    print('L\\'accuracy du jeu test est {}'.format(acc))\n",
    "    print('Matrice de confusion sur le jeu test:')\n",
    "    print(confusion_matrix(yt[:,label], y_pred_t[test_label_strip.index, label].round()))\n",
    "    print('Rapport : ')\n",
    "    print(classification_report(yt[:,label], y_pred_t[test_label_strip.index, label].round()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
